{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGJo0GfOOKxFRh/BVVCXD/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashgunde/Heart-Disease-Predictor-AISC-/blob/main/HeartATTACK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "0RkLIAWEdBQb",
        "outputId": "89c90d7a-9f7a-46de-8039-f7dba4662daf"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow.keras.optimizer'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ed9f71f4ae22>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#Loads dataset into df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.optimizer'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from mmap import ACCESS_DEFAULT\n",
        "from os import access\n",
        "import numpy as np\n",
        "#Math library\n",
        "import pandas as pd\n",
        "#data science lib\n",
        "import matplotlib.pyplot as plt\n",
        "#Graphs\n",
        "import seaborn as sns\n",
        "#graphs\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.optimizer import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "#Loads dataset into df\n",
        "df = pd.read_csv(\"../lib/heart.csv\")\n",
        "\n",
        "df.head()\n",
        "df.shape\n",
        "\n",
        "df.sex.value_counts()\n",
        "\n",
        "#Countplot displaying \"Thal\" - A number that represents an assessment of blood flow during exercise\n",
        "sns.countplot(x=\"thal\", data=df, palette=\"bwr\")\n",
        "plt.show()\n",
        "\n",
        "#Countplot displaying the amount of males and females within a dataset\n",
        "sns.countplot(x=\"sex\", data = df, palette = \"mako_r\")\n",
        "plt.show()\n",
        "\n",
        "#Finds the amount of people within the dataset that have a slope of 2 (Downsloping slope of the peak)\n",
        "count2slope = len(df[df.slope == 2])\n",
        "print(\"Percentage of people that have a slope of 2: {:.2f}%\".format((count2slope/(len(df.slope)) * 100)))\n",
        "\n",
        "#Finds the amount of people that have a Fasting Blood Sugar <= 120\n",
        "count0fbs = len(df[df.fbs == 0])\n",
        "print(\"Percentage of people that have a Fasting Blood Sugar <= 120: {:.2f}%\".format((count0fbs/(len(df.fbs)) * 100)))\n",
        "\n",
        "#Finds the amount of people that have a Fasting Blood Sugar > 120\n",
        "count1fbs = len(df[df.fbs == 1])\n",
        "print(\"Percentage of people that have a Fasting Blood Sugar > 120: {:.2f}%\".format((count1fbs/(len(df.fbs)) * 100)))\n",
        "\n",
        "df.groupby('target').mean()\n",
        "#Gets mean values in the dataset depending on if the patient has heart disease or not\n",
        "\n",
        "pd.crosstab(df.age, df.thal).plot(kind =\"bar\", figsize = (20,6))\n",
        "plt.title(\"Thal Frequency for Ages\")\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "pd.crosstab(df.sex, df.thal).plot(kind =\"bar\", figsize = (25,9))\n",
        "plt.title(\"Thal Frequency for Genders\")\n",
        "plt.xlabel('Gender')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "pd.crosstab(df.slope, df.target).plot(kind =\"bar\", figsize = (15,6))\n",
        "plt.title(\"Target values for different slopes\")\n",
        "plt.xlabel('Slope')\n",
        "plt.ylabel('Target')\n",
        "plt.show()\n",
        "# A higher slope means you are more likely to have heart disease\n",
        "\n",
        "pd.crosstab(df.fbs, df.target).plot(kind =\"bar\", figsize = (15,6), color = ('black', 'red'))\n",
        "plt.title(\"Fasting Blood Sugar and Heart Disease\")\n",
        "plt.xlabel('Fasting Blood Sugar')\n",
        "plt.ylabel('Target')\n",
        "plt.legend(['No Heart Disease', 'Heart Disease'])\n",
        "plt.show()\n",
        "#Based on our graph there is no correlation\n",
        "\n",
        "pd.crosstab(df.cp, df.target).plot(kind =\"bar\", figsize = (20,6), color = ('yellow', 'purple'))\n",
        "plt.title(\"Heart Disease Dependent on Chest Pain Type\")\n",
        "plt.xlabel('Chest Pain Type')\n",
        "plt.ylabel('Target')\n",
        "plt.legend(['No Heart Disease', 'Heart Disease'])\n",
        "plt.show()\n",
        "#Based on our graph we can say that chest pain type is positively correlated with heart disease\n",
        "\n",
        "#Dummy Variables\n",
        "a = pd.get_dummies(df['cp'], prefix =\"cp\")\n",
        "b = pd.get_dummies(df['thal'], prefix =\"thal\")\n",
        "c = pd.get_dummies(df['slope'], prefix =\"slope\")\n",
        "\n",
        "frames = [df, a, b, c]\n",
        "df = pd.concat(frames, axis = 1)\n",
        "df.head()\n",
        "\n",
        "y = df.target.values\n",
        "x_data = df.drop(['target'], axis = 1)\n",
        "\n",
        "\n",
        "#Min-Max Normalization\n",
        "x = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data))\n",
        "\n",
        "\n",
        "#Split training and testing within our dataset\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state=0)\n",
        "\n",
        "\n",
        "#Fit our training and testing\n",
        "x_train = x_train.T\n",
        "y_train = y_train.T\n",
        "x_test = x_test.T\n",
        "y_test = y_test.T\n",
        "\n",
        "\n",
        "#Sigmoid Function - Converts any number to go between 0 and 1\n",
        "#Equation - 1 / (1 + e^(-z))\n",
        "def sigmoid(z):\n",
        " y_head = 1 / (1 + np.exp(-z))\n",
        " return y_head\n",
        "\n",
        "\n",
        "#Forward and Backward Propagation Function\n",
        "def forwardBackward(weight, bias, x_train, y_train, lambda_reg=0.01):\n",
        "  #Forward aspect\n",
        "\n",
        "\n",
        " y_head = sigmoid(np.dot(weight.T, x_train) + bias)\n",
        " #The loss measures how well the predicted values are with the actual values\n",
        " loss = -(y_train * np.log(y_head) + (1 - y_train) * np.log(1 - y_head))\n",
        " #Calculates the total loss\n",
        " cost = np.sum(loss) / x_train.shape[1] + (lambda_reg / 2) * np.sum(weight ** 2)\n",
        "\n",
        "\n",
        " #Backward\n",
        " #This line computes the gradient of the loss with respect to the weights. It tells us how much the loss would change if we made small adjustments to the weights.\n",
        " derivative_weight = np.dot(x_train, ((y_head - y_train).T)) / x_train.shape[1] + lambda_reg * weight\n",
        " #This line computes the gradient of the loss with respect to the bias term.\n",
        " derivative_bias = np.sum(y_head - y_train) / x_train.shape[1]\n",
        " gradients = {\"Derivative Weight\": derivative_weight, \"Derivative Bias\": derivative_bias}\n",
        "\n",
        "\n",
        " return cost, gradients\n",
        "#Prediction Function\n",
        "def predict(weight, bias, x_test):\n",
        " z = np.dot(weight.T,x_test) + bias\n",
        " y_head = sigmoid(z)\n",
        " y_prediction = np.zeros((1, x_test.shape[1]))\n",
        "\n",
        "\n",
        " for i in range(y_head.shape[1]):\n",
        "   if y_head[0, i] <= 0.5:\n",
        "     y_prediction[0, i] = 0\n",
        "   else:\n",
        "     y_prediction[0, i] = 1\n",
        " return y_prediction\n",
        "\n",
        "\n",
        "#Initializes weight and bias for our model\n",
        "def initialize(dimension):\n",
        " weight = np.full((dimension, 1), 0.01)\n",
        " bias = 0.0\n",
        " return weight, bias\n",
        "\n",
        "\n",
        "#Helps improve model performance and optimization process\n",
        "def scale_features(x_train, x_test):\n",
        " scaler = StandardScaler()\n",
        " x_train_scaled = scaler.fit_transform(x_train.T).T\n",
        " x_test_scaled = scaler.transform(x_test.T).T\n",
        " return x_train_scaled, x_test_scaled\n",
        "\n",
        "\n",
        "#Update Function\n",
        "def update(weight, bias, x_train, y_train, learningRate, iteration, lambda_reg=0.01):\n",
        " costList = []\n",
        " index = []\n",
        "\n",
        "\n",
        " for i in range(iteration):\n",
        "   cost, gradients = forwardBackward(weight, bias, x_train, y_train, lambda_reg)\n",
        "   weight -= learningRate * gradients[\"Derivative Weight\"]\n",
        "   bias -= learningRate * gradients[\"Derivative Bias\"]\n",
        "\n",
        "   costList.append(cost)\n",
        "index.append(i)\n",
        "parameters = {\"weight\": weight, \"bias\": bias}\n",
        "\n",
        "\n",
        "print(\"Iterations: \", iteration)\n",
        "print(\"Costs:\", cost)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " #Shows how the cost function changes as the weights and bias's are updated over iterations\n",
        " #Future - if the cost function is decreasing, our model is improving it's \"fit\" to our data - making better predictions\n",
        "\n",
        "plt.plot(index, costList)\n",
        "plt.xlabel(\"Number of Iterations\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.show()\n",
        "return parameters, gradients\n",
        "\n",
        "\n",
        "\n",
        "#Logistic Regression Function\n",
        "def logistic_regression(x_train, y_train, x_test, y_test, learningRate, iterations, lambda_reg=0.01):\n",
        "\n",
        "\n",
        " #Scale features\n",
        " x_train, x_test = scale_features(x_train, x_test)\n",
        "\n",
        "\n",
        " #Initialize Parameters\n",
        " dimension = x_train.shape[0]\n",
        " weight, bias = initialize(dimension)\n",
        "\n",
        "\n",
        " parameters, gradient = update(weight, bias, x_train, y_train, learningRate, iterations, lambda_reg=0.01)\n",
        "\n",
        "\n",
        " #Make Predictions on the test set\n",
        " y_prediction = predict(parameters[\"weight\"], parameters[\"bias\"], x_test)\n",
        "\n",
        "\n",
        " #Convert the probabilities into binary\n",
        " y_prediction_binary = (y_prediction > 0.5).astype(int)\n",
        "\n",
        "\n",
        " #Calculate the accuracy\n",
        "accuracy = np.mean(y_prediction_binary == y_test)\n",
        "print(\"Manual Test Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "\n",
        "\n",
        "logistic_regression(x_train, y_train, x_test, y_test, learningRate=0.01, iterations=100, lambda_reg=0.01)\n",
        "\n",
        "\n",
        "accuracies = {}\n",
        "\n",
        "\n",
        "lr = LogisticRegression()\n",
        "lr.fit(x_train.T, y_train.T)\n",
        "acc = lr.score(x_test.T, y_test.T) * 100\n",
        "\n",
        "\n",
        "accuracies['Logistic Regression'] = acc\n",
        "print(\"Test Accuracy {:.2f}%\".format(acc))\n",
        "\n",
        "x_train = x_train.T\n",
        "y_train = y_train.T\n",
        "x_test = x_test.T\n",
        "y_test = y_test.T\n",
        "\n",
        "\n",
        "#Decision Tree Classifier\n",
        "dtc = DecisionTreeClassifier()\n",
        "dtc.fit(x_train, y_train)\n",
        "acc = dtc.score[x_test, y_test] * 100\n",
        "accuracies['Decision Tree'] = acc\n",
        "print(\"Decision Tree Accuracy: (:.2f)%\".format(acc))\n",
        "\n",
        "#Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=1000, random_state=1)\n",
        "rf.fit(x_train, y_train)\n",
        "acc = rf.score(x_test, y_test) * 100\n",
        "accuracies['Random Forest'] = acc\n",
        "print(\"Random Forest Classifier Accuracy: {:.2f}%\".format(acc))\n",
        "\n",
        "#KNN Model - Finding closest neighbors\n",
        "knn = KNeighborsClassifier(n_neighbors = 2)\n",
        "knn.fit(x_train, y_train)\n",
        "prediction = knn.predict(x_test)\n",
        "acc = knn.score(x_test, y_test) * 100\n",
        "\n",
        "scoreList = [] #Hold the accuracies of teh model depending on how many neighbors we look at\n",
        "for i in range(1,20):\n",
        "  knn2 = KNeighborsClassifier(n_neighbors = i)\n",
        "  knn2.fit(x_train, y_train)\n",
        "  scoreList.append(knn2.score(x_test, y_test)*100)\n",
        "\n",
        "print(\"KNN Model Accuracy: {:2f}%\".format(acc))\n",
        "\n",
        "plt.plot(range(1,20), scoreList)\n",
        "plt.xticks(np.average(1, 20, 1))\n",
        "plt.xlabel(\"K-Value\")\n",
        "plt.ylabel(\"Accuracy Percentage\")\n",
        "plt.show()\n",
        "\n",
        "accuracies['KNN Model'] = acc\n",
        "\n",
        "#Support Vector Machine (SVM) Algorithm\n",
        "svm = SVC(random_state = 1)\n",
        "svm.fit(x_train, y_train)\n",
        "acc = svm.score(x_test, y_test)\n",
        "accuracies['SVM'] = acc\n",
        "print(\"SVM Algorithm ACCURACY: {:2f}%\".format(acc))\n",
        "\n",
        "#Naive Bayes\n",
        "nb = GaussianNB()\n",
        "nb.fit(x_train, y_train)\n",
        "acc = nb.score(x_test, y_train)\n",
        "accuracies['Naive Bayes'] = acc\n",
        "print(\"Naive Bayes Accuracy: {:2f}%\".format(acc))\n",
        "\n",
        "colors = [\"red\", \"yellow\", \"orange\", \"green\", \"blue\", \"purple\"]\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.yticks(np.arrange(0, 100, 10))\n",
        "plt.xlabel(\"Types of Algorithms\")\n",
        "plt.ylabel(\"Accuracy Percentage(%)\")\n",
        "sns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()), palette=colors)\n",
        "plt.show()\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(x, y, test_size = 0.2, random_state = None)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size = 0.5, random_state = None)\n",
        "\n",
        "#sequentialmodel\n",
        "model = Sequential()\n",
        "mdoel.add(Input(shape=(24,)))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#Apply Optimizer Algorithm\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics = ['accuracy'])\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience = 10)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {test_loss}')\n",
        "print(f'Test Accuracy: {test_accuracy}')\n",
        "\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = (y_pred_prob.flatten() > 0.5).astype(int)\n",
        "\n",
        "#Shows predicted values in binary bcompared to actual values within target values of our dataset\n",
        "print(f'Predictions: {y_pred}')\n",
        "print(f'True Values: {y_test.flatten()}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j2Is_T0V4w3y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}